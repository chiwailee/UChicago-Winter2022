{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import metrics\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sequences(seq_size, obs):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(obs)-SEQUENCE_SIZE):\n",
    "        #print(i)\n",
    "        window = obs[i:(i+SEQUENCE_SIZE)]\n",
    "        after_window = obs[i+SEQUENCE_SIZE]\n",
    "        window = [[x] for x in window]\n",
    "        #print(\"{} - {}\".format(window,after_window))\n",
    "        x.append(window)\n",
    "        y.append(after_window)\n",
    "        \n",
    "    return np.array(x),np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    #x = layers.GlobalAveragePooling2D(data_format=\"channels_first\")(x)\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_original = False\n",
    "if use_original:\n",
    "    names = ['year', 'month', 'day', 'dec_year', 'sn_value' , 'sn_error', 'obs_num', 'extra']\n",
    "    target_col = 'sn_value'\n",
    "    periods_per_day = 6\n",
    "\n",
    "    fn = \"https://data.heatonresearch.com/data/t81-558/SN_d_tot_V2.0.csv\"\n",
    "    df = pd.read_csv(fn,\n",
    "        sep=';',header=None,names=names,\n",
    "        na_values=['-1'], index_col=False)\n",
    "\n",
    "else:\n",
    "    names = ['date', 'year','close', 'target',]\n",
    "    target_col = 'target'\n",
    "    perdiods_per_day = 10\n",
    "    ticker = 'fvty'\n",
    "\n",
    "    #fn = \"c://data//transformer_data_heaton.csv\"\n",
    "    #fn = \"c://data//transformer_data_\" + ticker + \".csv\"\n",
    "    fn = \"c:/dev/github/UChicago/Winter2023/UChicago-Winter2023/transformers/transformer_data_\" + ticker + \".csv\"\n",
    "    df = pd.read_csv(fn, sep=',', na_values=['-1'], index_col=False)\n",
    "\n",
    "\n",
    "\n",
    "# Find the last zero and move one beyond\n",
    "#start_id = max(df[df['obs_num'] == 0].index.tolist())+1  \n",
    "#print(start_id)\n",
    "#df = df[start_id:] # Trim the rows that have missing observatio\n",
    "\n",
    "#df['sn_value'] = df['sn_value'].astype(float)\n",
    "\n",
    "#wrap a loop starting here and grab train and test data from the original df..\n",
    "\n",
    "windowsize = 7000\n",
    "stepsize   = 50\n",
    "periodsperday = 6\n",
    "perdiods_per_day = periodsperday\n",
    "input_fields = [target_col,'obs_num','sn_error']\n",
    "target_field = [target_col]\n",
    "input_fields = target_field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration x =  7000  to row-stepsize  7797\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 10, 1)       2           ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 10, 1)       7169        ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 10, 1)        0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 10, 1)       0           ['dropout[0][0]',                \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add[0][0]']   \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 10, 4)        8           ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 10, 4)        0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 10, 1)        5           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 10, 1)       0           ['conv1d_1[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 10, 1)       7169        ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 10, 1)        0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 10, 1)       0           ['dropout_2[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 10, 4)        8           ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 10, 4)        0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 10, 1)        5           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 10, 1)       0           ['conv1d_3[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 10, 1)       7169        ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 10, 1)        0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 10, 1)       0           ['dropout_4[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_4[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 10, 4)        8           ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 10, 4)        0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 10, 1)        5           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 10, 1)       0           ['conv1d_5[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_5[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 10, 1)       7169        ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 10, 1)        0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 10, 1)       0           ['dropout_6[0][0]',              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_6[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 10, 4)        8           ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 10, 4)        0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 10, 1)        5           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 10, 1)       0           ['conv1d_7[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 10)          0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          1408        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 30,281\n",
      "Trainable params: 30,281\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "88/88 [==============================] - 16s 137ms/step - loss: 9.9381e-04 - val_loss: 0.0013\n",
      "Epoch 2/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 9.9580e-04 - val_loss: 0.0013\n",
      "Epoch 3/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 9.7798e-04 - val_loss: 0.0013\n",
      "Epoch 4/200\n",
      "88/88 [==============================] - 12s 135ms/step - loss: 9.7239e-04 - val_loss: 0.0013\n",
      "Epoch 5/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 9.2641e-04 - val_loss: 0.0012\n",
      "Epoch 6/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 9.0980e-04 - val_loss: 0.0013\n",
      "Epoch 7/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 9.3477e-04 - val_loss: 0.0012\n",
      "Epoch 8/200\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 9.2836e-04 - val_loss: 0.0012\n",
      "Epoch 9/200\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 9.0823e-04 - val_loss: 0.0012\n",
      "Epoch 10/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 9.0684e-04 - val_loss: 0.0012\n",
      "Epoch 11/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 9.0833e-04 - val_loss: 0.0013\n",
      "Epoch 12/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 9.1453e-04 - val_loss: 0.0012\n",
      "Epoch 13/200\n",
      "88/88 [==============================] - 12s 134ms/step - loss: 9.1061e-04 - val_loss: 0.0012\n",
      "Epoch 14/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 9.0217e-04 - val_loss: 0.0012\n",
      "Epoch 15/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.6722e-04 - val_loss: 0.0012\n",
      "Epoch 16/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.8137e-04 - val_loss: 0.0012\n",
      "Epoch 17/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 9.0275e-04 - val_loss: 0.0012\n",
      "Epoch 18/200\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 8.9147e-04 - val_loss: 0.0012\n",
      "Epoch 19/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 9.0173e-04 - val_loss: 0.0012\n",
      "Epoch 20/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.7153e-04 - val_loss: 0.0012\n",
      "Epoch 21/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.9252e-04 - val_loss: 0.0012\n",
      "Epoch 22/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 8.5139e-04 - val_loss: 0.0012\n",
      "Epoch 23/200\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 8.7390e-04 - val_loss: 0.0012\n",
      "Epoch 24/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.6732e-04 - val_loss: 0.0012\n",
      "Epoch 25/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.4966e-04 - val_loss: 0.0012\n",
      "Epoch 26/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.7326e-04 - val_loss: 0.0012\n",
      "Epoch 27/200\n",
      "88/88 [==============================] - 11s 129ms/step - loss: 8.5269e-04 - val_loss: 0.0012\n",
      "Epoch 28/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 8.6907e-04 - val_loss: 0.0012\n",
      "Epoch 29/200\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 8.7403e-04 - val_loss: 0.0012\n",
      "Epoch 30/200\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 8.5771e-04 - val_loss: 0.0012\n",
      "Epoch 31/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.3813e-04 - val_loss: 0.0012\n",
      "Epoch 32/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 8.6348e-04 - val_loss: 0.0012\n",
      "Epoch 33/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.3584e-04 - val_loss: 0.0012\n",
      "Epoch 34/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.6343e-04 - val_loss: 0.0012\n",
      "Epoch 35/200\n",
      "88/88 [==============================] - 11s 128ms/step - loss: 8.3874e-04 - val_loss: 0.0012\n",
      "Epoch 36/200\n",
      "88/88 [==============================] - 11s 127ms/step - loss: 8.6653e-04 - val_loss: 0.0012\n",
      "Epoch 37/200\n",
      "88/88 [==============================] - 11s 126ms/step - loss: 8.5554e-04 - val_loss: 0.0012\n",
      "Epoch 38/200\n",
      "88/88 [==============================] - 11s 127ms/step - loss: 8.4657e-04 - val_loss: 0.0012\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 7.7309e-04\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "Score (RMSE): 0.027804493812938078\n",
      "time to execute =  0:07:23.843554\n",
      "Iteration x =  7050  to row-stepsize  7797\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 10, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 10, 1)       2           ['input_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 10, 1)       7169        ['layer_normalization_8[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 10, 1)        0           ['multi_head_attention_4[0][0]'] \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tf.__operators__.add_8 (TFOpLa  (None, 10, 1)       0           ['dropout_9[0][0]',              \n",
      " mbda)                                                            'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_8[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 10, 4)        8           ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 10, 4)        0           ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 10, 1)        5           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  (None, 10, 1)       0           ['conv1d_9[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_8[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_9[0][0]'] \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 10, 1)       7169        ['layer_normalization_10[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_5[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None, 10, 1)       0           ['dropout_11[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_9[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_10[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 10, 4)        0           ['conv1d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 10, 1)        5           ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  (None, 10, 1)       0           ['conv1d_11[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_10[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_11[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (None, 10, 1)       7169        ['layer_normalization_12[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_6[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  (None, 10, 1)       0           ['dropout_13[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_11[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_12[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 10, 4)        0           ['conv1d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 10, 1)        5           ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (TFOpL  (None, 10, 1)       0           ['conv1d_13[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_12[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_13[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (MultiH  (None, 10, 1)       7169        ['layer_normalization_14[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_7[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (TFOpL  (None, 10, 1)       0           ['dropout_15[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_13[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_14[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 10, 4)        0           ['conv1d_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 10, 1)        5           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (TFOpL  (None, 10, 1)       0           ['conv1d_15[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_14[0][0]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 10)          0           ['tf.__operators__.add_15[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          1408        ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 128)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            129         ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 30,281\n",
      "Trainable params: 30,281\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "88/88 [==============================] - 16s 137ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 2/200\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 3/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 9.8643e-04 - val_loss: 0.0013\n",
      "Epoch 4/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 9.6545e-04 - val_loss: 0.0013\n",
      "Epoch 5/200\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 9.4448e-04 - val_loss: 0.0013\n",
      "Epoch 6/200\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 9.6876e-04 - val_loss: 0.0013\n",
      "Epoch 7/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 9.3637e-04 - val_loss: 0.0013\n",
      "Epoch 8/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 9.2149e-04 - val_loss: 0.0013\n",
      "Epoch 9/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 9.3819e-04 - val_loss: 0.0013\n",
      "Epoch 10/200\n",
      "88/88 [==============================] - 12s 133ms/step - loss: 9.0260e-04 - val_loss: 0.0012\n",
      "Epoch 11/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 9.0903e-04 - val_loss: 0.0012\n",
      "Epoch 12/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.9835e-04 - val_loss: 0.0012\n",
      "Epoch 13/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.9357e-04 - val_loss: 0.0012\n",
      "Epoch 14/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.9895e-04 - val_loss: 0.0013\n",
      "Epoch 15/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 8.9380e-04 - val_loss: 0.0012\n",
      "Epoch 16/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 8.9998e-04 - val_loss: 0.0012\n",
      "Epoch 17/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.9226e-04 - val_loss: 0.0012\n",
      "Epoch 18/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.8540e-04 - val_loss: 0.0012\n",
      "Epoch 19/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.7780e-04 - val_loss: 0.0012\n",
      "Epoch 20/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.7294e-04 - val_loss: 0.0012\n",
      "Epoch 21/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.6206e-04 - val_loss: 0.0012\n",
      "Epoch 22/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.6568e-04 - val_loss: 0.0012\n",
      "Epoch 23/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.6541e-04 - val_loss: 0.0012\n",
      "Epoch 24/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 8.7581e-04 - val_loss: 0.0012\n",
      "Epoch 25/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.6181e-04 - val_loss: 0.0012\n",
      "Epoch 26/200\n",
      "88/88 [==============================] - 11s 129ms/step - loss: 8.6302e-04 - val_loss: 0.0013\n",
      "Epoch 27/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 8.6470e-04 - val_loss: 0.0012\n",
      "Epoch 28/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.7811e-04 - val_loss: 0.0012\n",
      "Epoch 29/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 8.4464e-04 - val_loss: 0.0012\n",
      "Epoch 30/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.6146e-04 - val_loss: 0.0012\n",
      "Epoch 31/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.6299e-04 - val_loss: 0.0012\n",
      "Epoch 32/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 8.3832e-04 - val_loss: 0.0012\n",
      "Epoch 33/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.3108e-04 - val_loss: 0.0012\n",
      "Epoch 34/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.4634e-04 - val_loss: 0.0012\n",
      "Epoch 35/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 8.5525e-04 - val_loss: 0.0012\n",
      "Epoch 36/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.5275e-04 - val_loss: 0.0012\n",
      "Epoch 37/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.8371e-04 - val_loss: 0.0012\n",
      "Epoch 38/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 8.3065e-04 - val_loss: 0.0012\n",
      "Epoch 39/200\n",
      "88/88 [==============================] - 11s 129ms/step - loss: 8.3920e-04 - val_loss: 0.0012\n",
      "Epoch 40/200\n",
      "88/88 [==============================] - 11s 129ms/step - loss: 8.2464e-04 - val_loss: 0.0012\n",
      "Epoch 41/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.2913e-04 - val_loss: 0.0012\n",
      "Epoch 42/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 8.3319e-04 - val_loss: 0.0012\n",
      "Epoch 43/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.3280e-04 - val_loss: 0.0012\n",
      "Epoch 44/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.3518e-04 - val_loss: 0.0012\n",
      "Epoch 45/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 8.2108e-04 - val_loss: 0.0012\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0016\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Score (RMSE): 0.040001955998602644\n",
      "time to execute =  0:16:07.605101\n",
      "Iteration x =  7100  to row-stepsize  7797\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 10, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 10, 1)       2           ['input_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (MultiH  (None, 10, 1)       7169        ['layer_normalization_16[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_8[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (TFOpL  (None, 10, 1)       0           ['dropout_18[0][0]',             \n",
      " ambda)                                                           'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_16[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv1d_16 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 10, 4)        0           ['conv1d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 10, 1)        5           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (TFOpL  (None, 10, 1)       0           ['conv1d_17[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_16[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_18 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_17[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, 10, 1)       7169        ['layer_normalization_18[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_9[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (TFOpL  (None, 10, 1)       0           ['dropout_20[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_17[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_19 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_18[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 10, 4)        0           ['conv1d_18[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 10, 1)        5           ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (TFOpL  (None, 10, 1)       0           ['conv1d_19[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_18[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_19[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, 10, 1)       7169        ['layer_normalization_20[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_10[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (TFOpL  (None, 10, 1)       0           ['dropout_22[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_19[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_21 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_20[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 10, 4)        0           ['conv1d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 10, 1)        5           ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (TFOpL  (None, 10, 1)       0           ['conv1d_21[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_20[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_22 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_21[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (Multi  (None, 10, 1)       7169        ['layer_normalization_22[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_11[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (TFOpL  (None, 10, 1)       0           ['dropout_24[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_21[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_23 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_22[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 10, 4)        0           ['conv1d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 10, 1)        5           ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (TFOpL  (None, 10, 1)       0           ['conv1d_23[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_22[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_2 (Gl  (None, 10)          0           ['tf.__operators__.add_23[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          1408        ['global_average_pooling1d_2[0][0\n",
      "                                                                 ]']                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 128)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            129         ['dropout_26[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 30,281\n",
      "Trainable params: 30,281\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "88/88 [==============================] - 16s 137ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 2/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 3/200\n",
      "88/88 [==============================] - 11s 129ms/step - loss: 9.7579e-04 - val_loss: 0.0013\n",
      "Epoch 4/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 9.5446e-04 - val_loss: 0.0013\n",
      "Epoch 5/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 9.3217e-04 - val_loss: 0.0013\n",
      "Epoch 6/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 9.4747e-04 - val_loss: 0.0013\n",
      "Epoch 7/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 9.2298e-04 - val_loss: 0.0013\n",
      "Epoch 8/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 9.1695e-04 - val_loss: 0.0013\n",
      "Epoch 9/200\n",
      "88/88 [==============================] - 11s 126ms/step - loss: 9.0814e-04 - val_loss: 0.0013\n",
      "Epoch 10/200\n",
      "88/88 [==============================] - 11s 126ms/step - loss: 9.1870e-04 - val_loss: 0.0013\n",
      "Epoch 11/200\n",
      "88/88 [==============================] - 11s 125ms/step - loss: 9.2420e-04 - val_loss: 0.0013\n",
      "Epoch 12/200\n",
      "88/88 [==============================] - 11s 125ms/step - loss: 9.0117e-04 - val_loss: 0.0013\n",
      "Epoch 13/200\n",
      "88/88 [==============================] - 11s 129ms/step - loss: 8.9199e-04 - val_loss: 0.0013\n",
      "Epoch 14/200\n",
      "88/88 [==============================] - 11s 128ms/step - loss: 9.0506e-04 - val_loss: 0.0013\n",
      "Epoch 15/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 9.0466e-04 - val_loss: 0.0013\n",
      "Epoch 16/200\n",
      "88/88 [==============================] - 11s 129ms/step - loss: 9.0067e-04 - val_loss: 0.0013\n",
      "Epoch 17/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 8.9905e-04 - val_loss: 0.0013\n",
      "Epoch 18/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.9537e-04 - val_loss: 0.0013\n",
      "Epoch 19/200\n",
      "88/88 [==============================] - 11s 129ms/step - loss: 8.6435e-04 - val_loss: 0.0013\n",
      "Epoch 20/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 8.6926e-04 - val_loss: 0.0013\n",
      "Epoch 21/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.8841e-04 - val_loss: 0.0013\n",
      "Epoch 22/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.7181e-04 - val_loss: 0.0013\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0014\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "Score (RMSE): 0.03748431586747759\n",
      "time to execute =  0:20:22.851620\n",
      "Iteration x =  7150  to row-stepsize  7797\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 10, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_24 (LayerN  (None, 10, 1)       2           ['input_4[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_12 (Multi  (None, 10, 1)       7169        ['layer_normalization_24[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_12[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (TFOpL  (None, 10, 1)       0           ['dropout_27[0][0]',             \n",
      " ambda)                                                           'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_25 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_24[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 10, 4)        0           ['conv1d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)             (None, 10, 1)        5           ['dropout_28[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (TFOpL  (None, 10, 1)       0           ['conv1d_25[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_24[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_26 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_25[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_13 (Multi  (None, 10, 1)       7169        ['layer_normalization_26[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_13[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_26 (TFOpL  (None, 10, 1)       0           ['dropout_29[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_25[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_27 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_26[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)           (None, 10, 4)        0           ['conv1d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)             (None, 10, 1)        5           ['dropout_30[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_27 (TFOpL  (None, 10, 1)       0           ['conv1d_27[0][0]',              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ambda)                                                           'tf.__operators__.add_26[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_28 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_27[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (Multi  (None, 10, 1)       7169        ['layer_normalization_28[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_31 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_14[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_28 (TFOpL  (None, 10, 1)       0           ['dropout_31[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_27[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_29 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_28[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_32 (Dropout)           (None, 10, 4)        0           ['conv1d_28[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)             (None, 10, 1)        5           ['dropout_32[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_29 (TFOpL  (None, 10, 1)       0           ['conv1d_29[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_28[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_30 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_29[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_15 (Multi  (None, 10, 1)       7169        ['layer_normalization_30[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_33 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_15[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_30 (TFOpL  (None, 10, 1)       0           ['dropout_33[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_29[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_31 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_30[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_30 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_34 (Dropout)           (None, 10, 4)        0           ['conv1d_30[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_31 (Conv1D)             (None, 10, 1)        5           ['dropout_34[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_31 (TFOpL  (None, 10, 1)       0           ['conv1d_31[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_30[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_3 (Gl  (None, 10)          0           ['tf.__operators__.add_31[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          1408        ['global_average_pooling1d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 128)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            129         ['dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 30,281\n",
      "Trainable params: 30,281\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "88/88 [==============================] - 16s 141ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 2/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 3/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 9.8905e-04 - val_loss: 0.0013\n",
      "Epoch 4/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 9.6174e-04 - val_loss: 0.0013\n",
      "Epoch 5/200\n",
      "88/88 [==============================] - 11s 128ms/step - loss: 9.2493e-04 - val_loss: 0.0013\n",
      "Epoch 6/200\n",
      "88/88 [==============================] - 11s 129ms/step - loss: 9.2282e-04 - val_loss: 0.0013\n",
      "Epoch 7/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 9.3875e-04 - val_loss: 0.0013\n",
      "Epoch 8/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 9.1936e-04 - val_loss: 0.0013\n",
      "Epoch 9/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 9.3783e-04 - val_loss: 0.0013\n",
      "Epoch 10/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 9.0888e-04 - val_loss: 0.0013\n",
      "Epoch 11/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.9374e-04 - val_loss: 0.0013\n",
      "Epoch 12/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 9.0002e-04 - val_loss: 0.0013\n",
      "Epoch 13/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 9.0457e-04 - val_loss: 0.0013\n",
      "Epoch 14/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.9092e-04 - val_loss: 0.0013\n",
      "Epoch 15/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 9.0091e-04 - val_loss: 0.0013\n",
      "Epoch 16/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.7796e-04 - val_loss: 0.0013\n",
      "Epoch 17/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 8.8973e-04 - val_loss: 0.0013\n",
      "Epoch 18/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.8221e-04 - val_loss: 0.0013\n",
      "Epoch 19/200\n",
      "88/88 [==============================] - 11s 131ms/step - loss: 8.9433e-04 - val_loss: 0.0013\n",
      "Epoch 20/200\n",
      "88/88 [==============================] - 11s 129ms/step - loss: 8.8288e-04 - val_loss: 0.0013\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 11s 130ms/step - loss: 8.6938e-04 - val_loss: 0.0013\n",
      "Epoch 22/200\n",
      "88/88 [==============================] - 11s 128ms/step - loss: 8.7194e-04 - val_loss: 0.0013\n",
      "Epoch 23/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 8.8040e-04 - val_loss: 0.0013\n",
      "Epoch 24/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 8.7445e-04 - val_loss: 0.0013\n",
      "Epoch 25/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 8.7999e-04 - val_loss: 0.0013\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0011\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Score (RMSE): 0.03245164929157824\n",
      "time to execute =  0:25:15.517621\n",
      "Iteration x =  7200  to row-stepsize  7797\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 10, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_32 (LayerN  (None, 10, 1)       2           ['input_5[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_16 (Multi  (None, 10, 1)       7169        ['layer_normalization_32[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_16[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_32 (TFOpL  (None, 10, 1)       0           ['dropout_36[0][0]',             \n",
      " ambda)                                                           'input_5[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_33 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_32[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_32 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 10, 4)        0           ['conv1d_32[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_33 (Conv1D)             (None, 10, 1)        5           ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_33 (TFOpL  (None, 10, 1)       0           ['conv1d_33[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_32[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_34 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_33[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_17 (Multi  (None, 10, 1)       7169        ['layer_normalization_34[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_17[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_34 (TFOpL  (None, 10, 1)       0           ['dropout_38[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_33[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_35 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_34[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_34 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)           (None, 10, 4)        0           ['conv1d_34[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_35 (Conv1D)             (None, 10, 1)        5           ['dropout_39[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_35 (TFOpL  (None, 10, 1)       0           ['conv1d_35[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_34[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_36 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_35[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_18 (Multi  (None, 10, 1)       7169        ['layer_normalization_36[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_40 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_18[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_36 (TFOpL  (None, 10, 1)       0           ['dropout_40[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_35[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_37 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_36[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_36 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)           (None, 10, 4)        0           ['conv1d_36[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_37 (Conv1D)             (None, 10, 1)        5           ['dropout_41[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_37 (TFOpL  (None, 10, 1)       0           ['conv1d_37[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_36[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_38 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_37[0][0]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_19 (Multi  (None, 10, 1)       7169        ['layer_normalization_38[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_42 (Dropout)           (None, 10, 1)        0           ['multi_head_attention_19[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_38 (TFOpL  (None, 10, 1)       0           ['dropout_42[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_37[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_39 (LayerN  (None, 10, 1)       2           ['tf.__operators__.add_38[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_38 (Conv1D)             (None, 10, 4)        8           ['layer_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_43 (Dropout)           (None, 10, 4)        0           ['conv1d_38[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_39 (Conv1D)             (None, 10, 1)        5           ['dropout_43[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_39 (TFOpL  (None, 10, 1)       0           ['conv1d_39[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_38[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_4 (Gl  (None, 10)          0           ['tf.__operators__.add_39[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          1408        ['global_average_pooling1d_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_44 (Dropout)           (None, 128)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            129         ['dropout_44[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 30,281\n",
      "Trainable params: 30,281\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "88/88 [==============================] - 16s 139ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 2/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 3/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 4/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 9.6718e-04 - val_loss: 0.0014\n",
      "Epoch 5/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 9.3484e-04 - val_loss: 0.0014\n",
      "Epoch 6/200\n",
      "88/88 [==============================] - 11s 130ms/step - loss: 9.4542e-04 - val_loss: 0.0014\n",
      "Epoch 7/200\n",
      "88/88 [==============================] - 12s 131ms/step - loss: 9.2957e-04 - val_loss: 0.0014\n",
      "Epoch 8/200\n",
      "88/88 [==============================] - 12s 132ms/step - loss: 9.1399e-04 - val_loss: 0.0013\n",
      "Epoch 9/200\n",
      "88/88 [==============================] - 9s 108ms/step - loss: 9.2110e-04 - val_loss: 0.0013\n",
      "Epoch 10/200\n",
      "88/88 [==============================] - 9s 103ms/step - loss: 9.3630e-04 - val_loss: 0.0013\n",
      "Epoch 11/200\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 9.2327e-04 - val_loss: 0.0013\n",
      "Epoch 12/200\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 8.8200e-04 - val_loss: 0.0013\n",
      "Epoch 13/200\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 8.8936e-04 - val_loss: 0.0013\n",
      "Epoch 14/200\n",
      "88/88 [==============================] - 9s 103ms/step - loss: 8.8984e-04 - val_loss: 0.0013\n",
      "Epoch 15/200\n",
      "88/88 [==============================] - 9s 103ms/step - loss: 8.9040e-04 - val_loss: 0.0013\n",
      "Epoch 16/200\n",
      "88/88 [==============================] - 9s 103ms/step - loss: 9.0434e-04 - val_loss: 0.0013\n",
      "Epoch 17/200\n",
      "88/88 [==============================] - 9s 103ms/step - loss: 9.0102e-04 - val_loss: 0.0013\n",
      "Epoch 18/200\n",
      "88/88 [==============================] - 9s 102ms/step - loss: 8.7404e-04 - val_loss: 0.0013\n",
      "Epoch 19/200\n",
      "88/88 [==============================] - 9s 103ms/step - loss: 8.6806e-04 - val_loss: 0.0013\n",
      "Epoch 20/200\n",
      "83/88 [===========================>..] - ETA: 0s - loss: 8.9403e-04"
     ]
    }
   ],
   "source": [
    "SEQUENCE_SIZE   = 10\n",
    "pnls    = []\n",
    "row,col = df.shape\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "for x in range(windowsize,row-stepsize,stepsize):\n",
    "    print(\"Iteration x = \",x,\" to row-stepsize \",row-stepsize)\n",
    "    df_train = df.iloc[x-windowsize:x,:]\n",
    "    df_test  = df.iloc[x:x+stepsize]\n",
    "    df = df.loc[:,input_fields]\n",
    "    spots_train = df_train[target_col].tolist()\n",
    "    spots_test = df_test[target_col].tolist()\n",
    "    #spots_train = df.apply(lambda x: x.tolist(), axis=1).tolist()\n",
    "    #spots_test = df.apply(lambda x: x.tolist(), axis=1).tolist()\n",
    "\n",
    "#    print(\"Training set has {} observations.\".format(len(spots_train)))\n",
    "#    print(\"Test set has {} observations.\".format(len(spots_test)))\n",
    "    x_train,y_train = to_sequences(SEQUENCE_SIZE,spots_train)\n",
    "    x_test,y_test   = to_sequences(SEQUENCE_SIZE,spots_test)\n",
    "\n",
    "#    print(\"Shape of training set: {}\".format(x_train.shape))\n",
    "#    print(\"Shape of test set: {}\".format(x_test.shape))\n",
    "\n",
    "    input_shape = x_train.shape[1:]\n",
    "#    print(input_shape)\n",
    "    \n",
    "    model = build_model(\n",
    "        \n",
    "        input_shape,\n",
    "        head_size=256,\n",
    "        num_heads=4,\n",
    "        ff_dim=4,\n",
    "        num_transformer_blocks=4,\n",
    "        mlp_units=[128],\n",
    "        mlp_dropout=0.4,\n",
    "        dropout=0.25,\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"mean_squared_error\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(patience=10, \\\n",
    "        restore_best_weights=True)]\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=200,\n",
    "        batch_size=64,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    finish = datetime.datetime.now()\n",
    "    \n",
    "    model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "    pred = model.predict(x_test)\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(\"Score (RMSE): {}\".format(score)) \n",
    "\n",
    "    total   = 0\n",
    "    counter = 0\n",
    "\n",
    "    for x in range(len(pred)):\n",
    "        if pred[x] > 0.0:\n",
    "            total = y_test[x]\n",
    "        elif pred[x] < 0.0:\n",
    "            total = -1*y_test[x]\n",
    "        pnls.append([x,pred[x],y_test[x],total])\n",
    "\n",
    "    print(\"time to execute = \",finish-start)\n",
    "\n",
    "dfpnl = pd.DataFrame(data = pnls, columns=['cumpnl'])\n",
    "dfpnl['pnl'] = dfpnl['cumpnl'] - dfpnl['cumpnl'].shift(1)\n",
    "\n",
    "dfpnl['cummax'] = dfpnl['cumpnl'].cummax()\n",
    "ret = dfpnl['pnl'].mean()*252\n",
    "vol = dfpnl['pnl'].std()*pow(252,0.5)\n",
    "dfpnl['dd'] = dfpnl['cummax'] - df['cumpnl']\n",
    "mdd = dfpnl['dd'].max()\n",
    "print(\"Return = \",ret)\n",
    "print(\"Vol    = \",vol)\n",
    "info =0\n",
    "if(vol>0):\n",
    "    info = ret/vol\n",
    "print(\"Info  = \",info)\n",
    "calmar=0\n",
    "if (mdd>0):\n",
    "    calmar = ret/mdd\n",
    "print(\"MDD  = \",mdd)\n",
    "print(\"Calmar = \",calmar)\n",
    "df['cumpnl'].plot()\n",
    "df['cummax'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n",
      "[10, array([-0.0005572], dtype=float32), 0.0296875, -0.0296875]\n"
     ]
    }
   ],
   "source": [
    "print(len(pnls))\n",
    "print(pnls[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of PNL =  640\n",
      "periods =  32000\n",
      "days =  5333.333333333333\n",
      "days in each step =  8.333333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"length of PNL = \",len(pnls))\n",
    "x = len(pnls)\n",
    "periods = x*stepsize\n",
    "print(\"periods = \",periods)\n",
    "days = periods/periodsperday\n",
    "print(\"days = \",days)\n",
    "daysperstep = stepsize/periodsperday\n",
    "print(\"days in each step = \",daysperstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index      pred    actual       pnl\n",
      "0      0  0.002288 -0.005468 -0.005468\n",
      "1      1 -0.000890  0.023436 -0.023436\n",
      "2      2 -0.002429 -0.000781  0.000781\n",
      "3      3 -0.000301 -0.006250  0.006250\n",
      "4      4  0.000369 -0.050000 -0.050000\n",
      "     index      pred    actual       pnl  switch    cumpnl\n",
      "635     35 -0.003892 -0.011719  0.011719      -1  0.893726\n",
      "636     36  0.001324 -0.002344 -0.002344       1  0.891382\n",
      "637     37  0.002768 -0.018749 -0.018749       1  0.872633\n",
      "638     38  0.002367 -0.011719 -0.011719       1  0.860914\n",
      "639     39 -0.000583  0.002343 -0.002343      -1  0.858571\n",
      "Return =  0.17886901041666664\n",
      "Vol    =  0.38644137672571766\n",
      "Info  =  0.4628619531692164\n",
      "MDD  =  0.94765975\n",
      "Calmar =  0.188748134989026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = ['index','pred','actual','pnl']\n",
    "dfpnl = pd.DataFrame(data = pnls, columns=cols)\n",
    "dfpnl['pred'] = dfpnl['pred'].apply(lambda x: x[0])\n",
    "print(dfpnl.head())\n",
    "\n",
    "dfpnl['switch'] = dfpnl['pred'].apply(lambda x  : 1 if x>0 else -1 )\n",
    "dfpnl['pnl'] = dfpnl['switch'] * dfpnl['actual']\n",
    "#dfpnl = dfpnl.assign(pnl = lambda r: r['actual'] if r['pred'] > 0   else -1* r['actual'])\n",
    "\n",
    "\n",
    "for index,row in dfpnl.iterrows():\n",
    "    row['pred2'] = row['pred']\n",
    "    wt = 1.0\n",
    "    if (row['pred']<0):\n",
    "        wt = -1.0\n",
    "    dfpnl.loc[index,'pnl'] = wt * dfpnl.loc[index,'actual']\n",
    "\n",
    "dfpnl['cumpnl'] = dfpnl['pnl'].cumsum()\n",
    "print(dfpnl.tail())\n",
    "dfpnl['cummax'] = dfpnl['cumpnl'].cummax()\n",
    "ret = dfpnl['pnl'].mean()*(252/1.89)\n",
    "vol = dfpnl['pnl'].std()*pow(252/1.89,0.5)\n",
    "dfpnl['dd'] = dfpnl['cummax'] - dfpnl['cumpnl']\n",
    "mdd = dfpnl['dd'].max()\n",
    "print(\"Return = \",ret)\n",
    "print(\"Vol    = \",vol)\n",
    "info =0\n",
    "if(vol>0):\n",
    "    info = ret/vol\n",
    "print(\"Info  = \",info)\n",
    "calmar=0\n",
    "if (mdd>0):\n",
    "    calmar = ret/mdd\n",
    "print(\"MDD  = \",mdd)\n",
    "print(\"Calmar = \",calmar)\n",
    "dfpnl['cumpnl'].plot()\n",
    "dfpnl['cummax'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpnl.to_csv(\"c://data//transformer_output_\" + ticker + \".csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
